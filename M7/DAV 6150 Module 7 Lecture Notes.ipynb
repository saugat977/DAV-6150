{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 7: Regression Modeling for Categorical  Response Variables\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 Review\n",
    "\n",
    "#### Be very careful when making decisions regarding which attributes or categories to discard from your data\n",
    "\n",
    "The categorical attributes within the NYSED data set provide very valuable information, e.g., there is a very strong relationship between dropout count and school district, between dropout count and county, etc. These relationships could have been discovered via appropriate EDA work. The response variable is Dropouts, so ALL categories could feasibly have been used for purposes of training a model. \n",
    "\n",
    "Arbitrarily discarding categorical data results in an enormous amount of valuable predictive information being removed from the dataset, which is never an appropriate outcome.\n",
    "\n",
    "#### \"Outliers\": Are they or aren't they ?\n",
    "\n",
    "During EDA and Data Preparation we need to be __VERY__ careful regarding our approach to identifying and treating suspected outliers. Be sure to ask yourself:\n",
    "\n",
    "- __Is my approach to identifying outliers appropriate for each of the attributes I believe may have outlying values__? Make sure you do some research / develop some domain knowledge relative to the data you are working with so that you are well-informed regarding the possible __valid__ data values for each of your attributes.\n",
    "\n",
    "\n",
    "- __If I choose to remove observations that I suspect may have outliers from the data set, am I discarding too many observations__ ? If your outlier analysis results in the discarding of a large percentage of the observations contained within a data set, make sure you have a strong set of supporting facts/ domain knowledge that justify your actions.\n",
    "\n",
    "\n",
    "- __Is there some other method (other than deletion) I can make use of when I have identified a likely outlier__? Think about why the supposed outlying values may be present within the data set: Are they actually valid values? Could they be the result of a data entry or computational error of some sort? Are there similar observations within the data set? Can we preserve otherwise valid data via the use of a well-founded imputation of a new value for the supposed outlier? etc.\n",
    "\n",
    "#### Be careful of \"Data Leakage\" when selecting attributes as explanatory variables\n",
    "\n",
    "\"In statistics and machine learning, __leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time__, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\" (SOURCE: https://en.wikipedia.org/wiki/Leakage_(machine_learning)). \n",
    "\n",
    "\n",
    "In the P1 data set, we are provided with pairs of attributes whose meaning/context is nearly identical, e.g., the __pct__ and __cnt__ attributes. Of most concern is the presence of the __dropout_pct__ attribute, which is both highly correlated with the response variable, but is also conveying / providing the same type of information found in the __dropout_cnt__ response variable. If we were attempting to predict __dropout_cnt__ and we ALREADY had access to the __dropout_pct__ data, we would have no need of constructing any type of predictive model, i.e., we could simply convert the percentage to a count using the other data provided within the dataset. This is an example of an obvious case of __data leakage__, i.e., if we were truly trying to predict __dropout_cnt__, it is highly unlikely that we would have already had access to an attribute that provided us with the __percentage__ of students that dropped out. As such, the __dropout_pct__ attribute should have been __EXCLUDED__ from your models.\n",
    "\n",
    "#### Use of the sm.GLM() function for Negative Binomial models requires that we derive a value for the 'alpha' parameter from our data.\n",
    "\n",
    "As is explained in the Assigned Reading from M6 (https://timeseriesreasoning.com/contents/negative-binomial-regression-model/, we need to calculate a value for alpha before training a negative binomial regression model. This article (which was highlighted in the __M6 Lecture Notes__ as an appropriate approach to use) https://dius.com.au/2017/08/03/using-statsmodels-glms-to-model-beverage-consumption/#cameron also provides a detailed explanation of how to calculate an appropriate value for alpha. Use of an arbitrary value for the 'alpha' parameter is never appropriate.\n",
    "\n",
    "#### Make sure you discuss + explain your model coefficients\n",
    "\n",
    "The \"directionality\" of model coefficients can provide a great deal of information we can use to help explain a model to others. Therefore, we should __always__ review and discuss the directionality of our model coefficients for purposes of explaining to others the effects that various explanatory variables have upon our response variable.\n",
    "\n",
    "#### An R^2 metric cannot serve as a basis of comparison between linear models and Poisson or Negative Binomial models\n",
    "\n",
    "As we learned in M5 and M6, R^2 is a metric that applies to linear regression models: It measures the strength of a LINEAR relationship between independent and dependent variables. As such it has no applicability whatsoever to the assessment of any predictions made by either negative binomial or Poisson regression models. When comparing models of different types, we need to rely upon common metrics, e.g., log likelihood scores, RMSE scores, etc.\n",
    "\n",
    "#### Would a linear regression model have been appropriate for this data?\n",
    "\n",
    "As was discussed during the Module 6 Live Session + Lecture Notes, __linear regression models can generate negative non-whole values__. Since our response variable was a cardinal non-negative integer, use of a linear regression model would not be appropriate for purposes of estimating the number of cases of wine likely to be sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Regression Modeling for Categorical Response Variables\n",
    "\n",
    "- __Binary Logistic Regression__: The response variable is a __binary categorical variable__, while the explanatory variables can be either continuous, discrete or binary (e.g., dummy variables created from the values of a categorical variable).\n",
    "\n",
    "\n",
    "- __Multinomial Logistic Regression (MLR)__: The response variable is a categorical variable having __more than two (2) possible values__, while the explanatory variables can be either continuous, discrete or binary. There are many types of MLR models, and the exact type of MLR model you choose to use will be dependent on whether your response variable is __ordinal__ or __nominal__ in nature.\n",
    "\n",
    "\n",
    "### Binary Logistic Regression\n",
    "\n",
    "In binary logistic regression, the response variable is __binary__, i.e., it contains data encoded as either '1' (e.g., 'True') or '0' (e.g., 'False'). In other words, logistic regression is used for __classification__ problems.\n",
    "\n",
    "Logistic regression finds the best fitting __mathematical model__ (not a simple linear equation) to describe the relationship between the binary response variable and the explanatory variable(s). The values it generates are the coefficients of a formula to predict a __logit transformation__ (aka \"__log odds\"__) which is the logarithm of the odds $p$ of the probability of the presence of the characteristic of interest. \n",
    "\n",
    "$logit(p) = ln(p/(1-p)) = b0 + b1x1 + b2x2 + .. + bnxn$\n",
    "\n",
    "where $p$ = the probability of the __presence__ of a characteristic,\n",
    "\n",
    "$(1-p)$ = the probability of the __abscence__ of a characteristic,\n",
    "\n",
    "$b0$ = is a constant value\n",
    "\n",
    "$b1, b2, ..,bn$ = the regression coefficients for the explanatory variables $x1, x2, .., xn$\n",
    "\n",
    "Effective binary logistic regression models contain little to no collinearity amongst the explanatory variables used.\n",
    "\n",
    "The explanatory variables should be linearly related to the log odds of the probability of the presence of the characteristic of interest (i.e., the value of the response variable)\n",
    "\n",
    "Logistic regression generally does not work well when applied to very small data sets. For observational studies, a minimum of __500 samples/observations__ is recommended.\n",
    "\n",
    "Further explanation is available here:\n",
    "\n",
    "https://www.statisticssolutions.com/what-is-logistic-regression/\n",
    "\n",
    "\n",
    "\n",
    "### Multinomial Logistic Regression\n",
    "\n",
    "In multinomial logistic regression (MLR), the response variable is __multiclass__, i.e., it has __more than 2 possible values__. Multinomial regression calculates the probability of any observation being assigned to each of the possible classes. Therefore, the regression model must perform at least $n-1$ calculations, where $n$ is the number of possible classifications for the response variable.\n",
    "\n",
    "Why $n-1$ calculations? If we have $n$ classes, we can determine the probability of the $n$th class by simply summing the probabilities for each of the other classifications and subtracting that sum from $1$, i.e., \n",
    "\n",
    "### $P(Class_n) = 1 - ( P(Class_1) + P(Class_2) + ... + P(Class_{n-1}) ) $\n",
    "\n",
    "As with binary logistic regression, the coefficients of a MLR model predict the __log odds__ of the probability of any observation belonging to each of the $n$ classes. \n",
    "\n",
    "The probability of observing class $k$ out of $n$ total classes is:\n",
    "\n",
    "# $ P(y_i = k) = \\frac { e^{S_k} }  {e^{S_1} + e^{S_2} + ... + e^{S_n} } $\n",
    "\n",
    "(see https://medium.com/towards-data-science/understanding-logistic-regression-coefficients-7a719ebebd35)\n",
    "\n",
    "When performing multiple logistic regression, the algorithm assigns a predicted classification for a given observation based on __which of the possible classifications achieves the largest log odds value__.\n",
    "\n",
    "\n",
    "### How to interpret logistic regression model coefficients\n",
    "\n",
    "\"A one unit increase in explanatory variable $x$ (with all other explanatory variables held constant) increases the log-odds of response variable $y$ by the amount indicated by the coefficient of $x$.\"\n",
    "\n",
    "However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
    "\n",
    "\n",
    "### How to convert log odds to a probability\n",
    "\n",
    "- __Step 1__: Convert the log odds value to odds via __exponentiation__, i.e., the antilog of a log value. Assuming the log odds value we've been given is a __natural log__, we apply an __exponential function__ to convert the log odds value to and odds value.\n",
    "\n",
    "\n",
    "- __Step 2__: Calculate the probability as follows: \n",
    "\n",
    "### $ Prob = \\frac {odds} {1 + odds} $\n",
    "\n",
    "http://www.pmean.com/13/predicted.html\n",
    "\n",
    "\n",
    "### Advantages of Logistic Regression\n",
    "\n",
    "- Relatively computationally efficient + easy to implement\n",
    "\n",
    "\n",
    "- Relatively easy to interpret (though not as easy to interpret as linear regression models)\n",
    "\n",
    "\n",
    "- Can be effective even if features have not been scaled to similar ranges\n",
    "\n",
    "\n",
    "- Due to its simplicity, logistic regression is widely used as a baseline when comparing the performance of other more complex classification methods (e.g., decision trees, random forest, KNN, neural networks, etc.)\n",
    "\n",
    "\n",
    "### Disadvantages of Logistic Regression\n",
    "\n",
    "- Due to its simplicity, performance can lag behind that of more complex classification methods\n",
    "\n",
    "\n",
    "- Requires a __linear__ relationship between the explanatory variables and the response variable since it relies on a linear decision space\n",
    "\n",
    "\n",
    "- Requires that explanatory variables are __very__ independent of one another + that they have a substantive relationship with the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Logistic Regression Model Performance\n",
    "\n",
    "In __Module 5__ we learned about a wide variety of classification model performance metrics, all of which can be used for purposes of evaluating the performance of any type of logistic regression model:\n",
    "\n",
    "- Confusion Matrices: True Positives (TP), False Positives (FP), True Negatives (TN), False Negatives\n",
    " (FN)\n",
    " \n",
    " \n",
    "- Accuracy\n",
    "\n",
    "\n",
    "- Precision \n",
    "\n",
    "\n",
    "- Recall / Sensitivity\n",
    "\n",
    "\n",
    "- Specificity\n",
    "\n",
    "\n",
    "- F1 Score\n",
    "\n",
    "\n",
    "- ROC: __ROC (Receiver Operating Characteristic) Curve__: Calculate by plotting the __true positive rate (TPR)__ against the __false positive rate (FPR)__; http://www.saedsayad.com/model_evaluation_c.htm. Plotting TPR vs. FPR for a series of classification models constructed using different thresholds for predicting classifications yields a curve within a two-dimensional plane.\n",
    "\n",
    "\n",
    "- AUC: In a ROC plot, AUC is determined by calculating the area in the plot that falls __below / to the right of__ the ROC curve.  A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. The higher the AUC score, the better the performance of a model.  AUC scores from different models can be compared against one another to help us determine which model has the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression example with scikit-learn\n",
    "\n",
    "From the \"Python For Data Analysis\" textbook (from your DAV 5400 course): Use the 'Titanic' data set which contains passenger survival rate data for the ocean-going vessel of that name that sank in 1912. \n",
    "\n",
    "Our goal is to __try to predict whether or not a passenger would have been more likely than not to survive the sinking of the Titanic__ based on the data contained in the data set (i.e., build a __predictive model__).\n",
    "\n",
    "- First, we will fit a logistic regression model on a __training__ data set using the data set's __PClass__, __Age__, and __Sex__ attributes.\n",
    "\n",
    "\n",
    "- Then we evaluate the model using a __testing__ data set that is exclusive of the data contained in the __training__ data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the LogisticRegression() function from sklearn's 'linear_model' sub-library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load the training + testing data sets for the Titanic data\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/test.csv')\n",
    "\n",
    "# display the first four rows of the data set\n",
    "train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>IsFemale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                              Name     Sex   Age  SibSp  \\\n",
       "0          892       3                  Kelly, Mr. James    male  34.5      0   \n",
       "1          893       3  Wilkes, Mrs. James (Ellen Needs)  female  47.0      1   \n",
       "2          894       2         Myles, Mr. Thomas Francis    male  62.0      0   \n",
       "3          895       3                  Wirz, Mr. Albert    male  27.0      0   \n",
       "\n",
       "   Parch  Ticket    Fare Cabin Embarked  IsFemale  \n",
       "0      0  330911  7.8292   NaN        Q         0  \n",
       "1      0  363272  7.0000   NaN        S         1  \n",
       "2      0  240276  9.6875   NaN        Q         0  \n",
       "3      0  315154  8.6625   NaN        S         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the first four rows of the test data\n",
    "# Note the lack of a \"Survived\" indicator !!!\n",
    "test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many records in the training data set?\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many people survived in the training set?\n",
    "train.Survived.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3838383838383838"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage of the training set survived?\n",
    "train.Survived.values.sum() / train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: Since we know that 38.3% of the people in the training set survived, we could achieve a training model accuracy of (1 - .383) = 61.7% by simply predicting \"Did not survive\" for each passenger. This metric is referred to as the __null error rate__.  When evaluating the performance of a binary logistic regression model, always check to see whether the accuracy you are attaining exceeds the __null error rate__. If not, your model is unlikely to be of any value.\n",
    "\n",
    "\n",
    "Missing data will generally prevent the fitting of a model via either __scikit-learn__ or __statsmodels__, so we must first check both the training and testing data for missing data values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the training data for null values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the training data, the __Age__, __Cabin__ and __Embarked__ attributes all have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test data for null values\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the testing data, the __Age__, __Fare__ and __Cabin__  attributes all have missing data.\n",
    "\n",
    "So if we want to use __Age__ as part of our predictive model, we need to somehow fill the missing __Age__ values. In this instance, the author of the PfDA text chooses to use the relatively simple process of filling the missing values with the __median__ Age value found within the __training__ data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the median 'Age' value within the training data set\n",
    "impute_value = train['Age'].median()\n",
    "\n",
    "# now fill the missing 'Age' values in both the training and testing data sets\n",
    "train['Age'] = train['Age'].fillna(impute_value)\n",
    "test['Age'] = test['Age'].fillna(impute_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, __we create a dummy indicator for the 'Sex' categorical variable__: the new dummy variable 'IsFemale' contains a '1' if the 'Sex' value for a passenger is 'Female' and a '0' otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy variable for the 'Sex' attribute in both the training and\n",
    "# testing data sets\n",
    "train['IsFemale'] = (train['Sex'] == 'female').astype(int)\n",
    "test['IsFemale'] = (test['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our predictive model using the attributes we want to use as explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  0., 22.],\n",
       "       [ 1.,  1., 38.],\n",
       "       [ 3.,  1., 26.],\n",
       "       [ 1.,  1., 35.],\n",
       "       [ 3.,  0., 35.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a vector containing the names of the attributes to use\n",
    "predictors = ['Pclass', 'IsFemale', 'Age']\n",
    "\n",
    "# create a subset of the training data using ONLY the selected explanatory variables\n",
    "X_train = train[predictors].values\n",
    "\n",
    "# create a subset of the testing data using ONLY the selected explanatory variables\n",
    "X_test = test[predictors].values\n",
    "\n",
    "# isolate the response indicator for the training data\n",
    "y_train = train['Survived'].values\n",
    "\n",
    "# sanity check on training data\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check on response indicator\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James T\\Anaconda3\\envs\\DAV5400\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're using the LogisticRegression() method for this model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model: X_train contains our explanatory variables while \n",
    "# y_train contains the response variable\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7946127946127947"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the accuracy of the model relative to the training data set\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above we calculated the __null error rate__ for our data to be 61.7%. The model we've generated has an accuracy score of 79.46%. As such, our model appears to be useful.\n",
    "\n",
    "__What can we learn by examining the regression model coefficients for the explanatory variables?__\n",
    "\n",
    "While the actual numeric values of the coefficients of a regression model are often very difficult to interpret, we can use the fact that a coefficient is either positive or negative to determine what effect of an __increase__ in the value of a given variable will have on the predicted value of the response variable. \n",
    "\n",
    "In this binary logistic regression example, the value of the response variable is the \"log odds\" of the binary outcome being either a '0' or '1' ('0' meaning the passenger was more likely to perish while '1' indicates the passenger was more likely to survive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'IsFemale', 'Age']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.07373582,  2.52093733, -0.02864864]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the model coefficients for the explanatory variables\n",
    "print(predictors)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that:\n",
    "\n",
    "- __Passenger Class__: An increase in the value of 'Passenger_Class' is associated with a __decreased__ likelihood of survival, i.e., passengers in lower classes of service were more likely to perish than were passengers in relatively higher classes of service.\n",
    "\n",
    "\n",
    "- __IsFemale__: Being female increased the likelihood of survival\n",
    "\n",
    "\n",
    "- __Age__: The older the passenger, the less likely they were to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions for the test data using our new model\n",
    "y_predict = model.predict(X_test)\n",
    "y_predict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'y_predict' array contains predictions that answer the question: \"__Was a given passenger more likely than not to have survived the sinking of the Titanic?__\"\n",
    "\n",
    "If we had the actual __survived/perished__ indicators for the test data, we could now check the performance of the model via a variety of error metrics (e.g., accuracy, specificity, precision, recall, AUC, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Logistic Regression with scikit-learn\n",
    "\n",
    "https://nbviewer.jupyter.org/gist/justmarkham/6d5c061ca5aee67c4316471f8c2ae976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 Assignment Guidelines / Requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
